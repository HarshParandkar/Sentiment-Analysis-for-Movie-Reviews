# -*- coding: utf-8 -*-
"""IMDB Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FP1UewylM82rGzfbIcn_Bknxnk2Bu43m

IMPORT LIBRARIES
"""

import pandas as pd

!pip install tensorflow datasets

import tensorflow as tf
from tensorflow.keras import layers, models
from datasets import load_dataset
import matplotlib.pyplot as plt
import numpy as np

"""Load the Dataset named "imdb"
"""

dataset=load_dataset("imdb")

train_data=dataset["train"]
test_data=dataset['test']

train_data
train_data[0]

test_data
test_data[0]

"""TOKENIZING THE DATA"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = 10000
max_length = 200
embedding_dim = 16
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_data['text'])

train_sequences = tokenizer.texts_to_sequences(train_data['text'])
train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

test_sequences = tokenizer.texts_to_sequences(test_data['text'])
test_padded = pad_sequences(test_sequences, maxlen=max_length)

"""Building the Model"""

model = models.Sequential([
    layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    layers.GlobalAveragePooling1D(),
    layers.Dense(24, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

"""Training the model"""

num_epochs = 10

history = model.fit(
    np.array(train_padded),
    np.array(train_data['label']),
    epochs=num_epochs,
    validation_data=(np.array(test_padded), np.array(test_data['label']))
)

"""Evaluation of the built model"""

results = model.evaluate(np.array(test_padded), np.array(test_data['label']))
print(f"Test Loss: {results[0]}")
print(f"Test Accuracy: {results[1]}")

"""VISUALIZATION"""

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('NeuralEpoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label = 'val_loss')
plt.xlabel('NeuralEpoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.show()

"""SAVE THE MODEL"""

model.save('imdb_sentiment_model.h5')

from google.colab import files

files.download('imdb_sentiment_model.h5')

"""DOWNLOADING THE TOKENIZER PICKLE FILE
Creating Training Data again, then saving the pickle file.

"""

import pandas as pd

df = pd.read_csv('/content/imdb.csv')

training_texts = df['review'].tolist()

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(training_texts)

import pickle

# Saving the tokenizer
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)